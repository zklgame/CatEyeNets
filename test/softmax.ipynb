{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getcwd() + '/..')\n",
    "\n",
    "# Run some setup code for this notebook\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.data_utils import load_CIFAR10\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((49000, 3073), (1000, 3073), (1000, 3073), (500, 3073))\n"
     ]
    }
   ],
   "source": [
    "# Load the raw CIFAR-10 data\n",
    "cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# Split the data\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "mask = range(num_training, num_training+num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "mask = xrange(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "X_dev = X_dev.reshape(X_dev.shape[0], -1)\n",
    "\n",
    "# Normalize the data: subtract the mean rows\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "# append the bias dimension of ones\n",
    "X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))\n",
    "X_val = np.hstack((X_val, np.ones((X_val.shape[0], 1))))\n",
    "X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1))))\n",
    "X_dev= np.hstack((X_dev, np.ones((X_dev.shape[0], 1))))\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.386956\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# softmax_loss_vectorized function\n",
    "from classifiers.linear_classifier import softmax_loss_vectorized\n",
    "import time\n",
    "\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_vectorized(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *W = 0*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.736112, analytic: 0.736112, relative error: 4.939844e-08\n",
      "numerical: 1.359312, analytic: 1.359312, relative error: 2.223424e-08\n",
      "numerical: 3.672828, analytic: 3.672827, relative error: 2.815123e-08\n",
      "numerical: 5.150041, analytic: 5.150041, relative error: 1.513723e-08\n",
      "numerical: 0.439351, analytic: 0.439351, relative error: 5.945770e-08\n",
      "numerical: 3.621422, analytic: 3.621422, relative error: 2.221157e-08\n",
      "numerical: 1.943753, analytic: 1.943752, relative error: 5.199429e-08\n",
      "numerical: 0.220881, analytic: 0.220881, relative error: 4.691754e-07\n",
      "numerical: -9.092706, analytic: -9.092706, relative error: 4.104540e-09\n",
      "numerical: 0.378179, analytic: 0.378179, relative error: 3.649779e-08\n",
      "numerical: -1.764118, analytic: -1.764119, relative error: 2.513974e-08\n",
      "numerical: 3.271042, analytic: 3.271042, relative error: 1.854753e-08\n",
      "numerical: -1.781404, analytic: -1.781404, relative error: 3.165128e-08\n",
      "numerical: -2.152419, analytic: -2.152419, relative error: 5.668960e-09\n",
      "numerical: 0.350985, analytic: 0.350985, relative error: 2.059407e-07\n",
      "numerical: -0.787027, analytic: -0.787027, relative error: 5.968350e-08\n",
      "numerical: -1.181319, analytic: -1.181319, relative error: 1.497601e-09\n",
      "numerical: -0.426054, analytic: -0.426054, relative error: 6.143153e-08\n",
      "numerical: 4.932407, analytic: 4.932407, relative error: 2.360830e-08\n",
      "numerical: -6.181916, analytic: -6.181916, relative error: 4.386489e-09\n"
     ]
    }
   ],
   "source": [
    "loss, grad = softmax_loss_vectorized(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "from utils.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_vectorized(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "loss, grad = softmax_loss_vectorized(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_vectorized(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 5.890208\n",
      "iteration 100 / 5000: loss 3.194921\n",
      "iteration 200 / 5000: loss 2.332129\n",
      "iteration 300 / 5000: loss 2.300826\n",
      "iteration 400 / 5000: loss 2.217096\n",
      "iteration 500 / 5000: loss 2.161462\n",
      "iteration 600 / 5000: loss 2.104302\n",
      "iteration 700 / 5000: loss 2.125867\n",
      "iteration 800 / 5000: loss 2.038327\n",
      "iteration 900 / 5000: loss 2.169287\n",
      "iteration 1000 / 5000: loss 2.121470\n",
      "iteration 1100 / 5000: loss 1.999580\n",
      "iteration 1200 / 5000: loss 2.106824\n",
      "iteration 1300 / 5000: loss 1.826786\n",
      "iteration 1400 / 5000: loss 2.143686\n",
      "iteration 1500 / 5000: loss 2.039867\n",
      "iteration 1600 / 5000: loss 1.956495\n",
      "iteration 1700 / 5000: loss 1.888666\n",
      "iteration 1800 / 5000: loss 1.908971\n",
      "iteration 1900 / 5000: loss 1.861310\n",
      "iteration 2000 / 5000: loss 1.938574\n",
      "iteration 2100 / 5000: loss 1.838085\n",
      "iteration 2200 / 5000: loss 1.940508\n",
      "iteration 2300 / 5000: loss 2.019789\n",
      "iteration 2400 / 5000: loss 1.789948\n",
      "iteration 2500 / 5000: loss 1.925156\n",
      "iteration 2600 / 5000: loss 1.902407\n",
      "iteration 2700 / 5000: loss 1.909397\n",
      "iteration 2800 / 5000: loss 1.825632\n",
      "iteration 2900 / 5000: loss 1.824641\n",
      "iteration 3000 / 5000: loss 1.918740\n",
      "iteration 3100 / 5000: loss 1.795331\n",
      "iteration 3200 / 5000: loss 1.759469\n",
      "iteration 3300 / 5000: loss 1.543473\n",
      "iteration 3400 / 5000: loss 1.920789\n",
      "iteration 3500 / 5000: loss 1.774508\n",
      "iteration 3600 / 5000: loss 1.837029\n",
      "iteration 3700 / 5000: loss 1.755523\n",
      "iteration 3800 / 5000: loss 1.779667\n",
      "iteration 3900 / 5000: loss 1.855261\n",
      "iteration 4000 / 5000: loss 1.892036\n",
      "iteration 4100 / 5000: loss 1.710696\n",
      "iteration 4200 / 5000: loss 1.833561\n",
      "iteration 4300 / 5000: loss 1.887569\n",
      "iteration 4400 / 5000: loss 1.957419\n",
      "iteration 4500 / 5000: loss 1.837813\n",
      "iteration 4600 / 5000: loss 1.849479\n",
      "iteration 4700 / 5000: loss 1.850004\n",
      "iteration 4800 / 5000: loss 1.685684\n",
      "iteration 4900 / 5000: loss 1.766273\n",
      "lr 9.000000e-07 reg 0.000000e+00 train_accuracy: 0.395347 val_accuracy: 0.354000\n",
      "iteration 0 / 5000: loss 5.553815\n",
      "iteration 100 / 5000: loss 3.221160\n",
      "iteration 200 / 5000: loss 2.668618\n",
      "iteration 300 / 5000: loss 2.629958\n",
      "iteration 400 / 5000: loss 2.563990\n",
      "iteration 500 / 5000: loss 2.665957\n",
      "iteration 600 / 5000: loss 2.423917\n",
      "iteration 700 / 5000: loss 2.546606\n",
      "iteration 800 / 5000: loss 2.348642\n",
      "iteration 900 / 5000: loss 2.167254\n",
      "iteration 1000 / 5000: loss 2.351205\n",
      "iteration 1100 / 5000: loss 2.267056\n",
      "iteration 1200 / 5000: loss 2.321990\n",
      "iteration 1300 / 5000: loss 2.576225\n",
      "iteration 1400 / 5000: loss 2.256163\n",
      "iteration 1500 / 5000: loss 2.128683\n",
      "iteration 1600 / 5000: loss 2.284987\n",
      "iteration 1700 / 5000: loss 2.328405\n",
      "iteration 1800 / 5000: loss 2.172751\n",
      "iteration 1900 / 5000: loss 2.018442\n",
      "iteration 2000 / 5000: loss 2.195081\n",
      "iteration 2100 / 5000: loss 2.128819\n",
      "iteration 2200 / 5000: loss 2.259494\n",
      "iteration 2300 / 5000: loss 2.092245\n",
      "iteration 2400 / 5000: loss 2.162120\n",
      "iteration 2500 / 5000: loss 2.129752\n",
      "iteration 2600 / 5000: loss 2.122928\n",
      "iteration 2700 / 5000: loss 1.953368\n",
      "iteration 2800 / 5000: loss 2.069894\n",
      "iteration 2900 / 5000: loss 2.004992\n",
      "iteration 3000 / 5000: loss 2.100304\n",
      "iteration 3100 / 5000: loss 2.252609\n",
      "iteration 3200 / 5000: loss 1.952120\n",
      "iteration 3300 / 5000: loss 2.008417\n",
      "iteration 3400 / 5000: loss 1.922647\n",
      "iteration 3500 / 5000: loss 2.000243\n",
      "iteration 3600 / 5000: loss 1.998615\n",
      "iteration 3700 / 5000: loss 2.134564\n",
      "iteration 3800 / 5000: loss 1.965767\n",
      "iteration 3900 / 5000: loss 2.208995\n",
      "iteration 4000 / 5000: loss 1.961832\n",
      "iteration 4100 / 5000: loss 1.910969\n",
      "iteration 4200 / 5000: loss 2.143085\n",
      "iteration 4300 / 5000: loss 2.004921\n",
      "iteration 4400 / 5000: loss 1.953204\n",
      "iteration 4500 / 5000: loss 2.026110\n",
      "iteration 4600 / 5000: loss 1.734592\n",
      "iteration 4700 / 5000: loss 1.899200\n",
      "iteration 4800 / 5000: loss 1.876720\n",
      "iteration 4900 / 5000: loss 1.875525\n",
      "lr 9.000000e-07 reg 1.000000e+01 train_accuracy: 0.394551 val_accuracy: 0.355000\n",
      "iteration 0 / 5000: loss 5.869720\n",
      "iteration 100 / 5000: loss 3.382915\n",
      "iteration 200 / 5000: loss 3.116546\n",
      "iteration 300 / 5000: loss 2.858616\n",
      "iteration 400 / 5000: loss 2.839334\n",
      "iteration 500 / 5000: loss 2.961940\n",
      "iteration 600 / 5000: loss 2.865022\n",
      "iteration 700 / 5000: loss 2.712839\n",
      "iteration 800 / 5000: loss 2.559554\n",
      "iteration 900 / 5000: loss 2.548663\n",
      "iteration 1000 / 5000: loss 2.483321\n",
      "iteration 1100 / 5000: loss 2.422072\n",
      "iteration 1200 / 5000: loss 2.391160\n",
      "iteration 1300 / 5000: loss 2.348792\n",
      "iteration 1400 / 5000: loss 2.642693\n",
      "iteration 1500 / 5000: loss 2.357631\n",
      "iteration 1600 / 5000: loss 2.317158\n",
      "iteration 1700 / 5000: loss 2.586897\n",
      "iteration 1800 / 5000: loss 2.315577\n",
      "iteration 1900 / 5000: loss 2.321095\n",
      "iteration 2000 / 5000: loss 2.434012\n",
      "iteration 2100 / 5000: loss 2.309223\n",
      "iteration 2200 / 5000: loss 2.451501\n",
      "iteration 2300 / 5000: loss 2.388473\n",
      "iteration 2400 / 5000: loss 2.446214\n",
      "iteration 2500 / 5000: loss 2.350929\n",
      "iteration 2600 / 5000: loss 2.384842\n",
      "iteration 2700 / 5000: loss 2.159958\n",
      "iteration 2800 / 5000: loss 2.231560\n",
      "iteration 2900 / 5000: loss 2.258436\n",
      "iteration 3000 / 5000: loss 2.217510\n",
      "iteration 3100 / 5000: loss 2.047609\n",
      "iteration 3200 / 5000: loss 2.270681\n",
      "iteration 3300 / 5000: loss 2.204050\n",
      "iteration 3400 / 5000: loss 2.258115\n",
      "iteration 3500 / 5000: loss 2.330980\n",
      "iteration 3600 / 5000: loss 2.298748\n",
      "iteration 3700 / 5000: loss 2.146381\n",
      "iteration 3800 / 5000: loss 2.090754\n",
      "iteration 3900 / 5000: loss 2.260914\n",
      "iteration 4000 / 5000: loss 2.259010\n",
      "iteration 4100 / 5000: loss 2.358499\n",
      "iteration 4200 / 5000: loss 2.152777\n",
      "iteration 4300 / 5000: loss 2.115279\n",
      "iteration 4400 / 5000: loss 2.148883\n",
      "iteration 4500 / 5000: loss 2.126283\n",
      "iteration 4600 / 5000: loss 2.079936\n",
      "iteration 4700 / 5000: loss 2.195484\n",
      "iteration 4800 / 5000: loss 2.104593\n",
      "iteration 4900 / 5000: loss 2.087083\n",
      "lr 9.000000e-07 reg 2.000000e+01 train_accuracy: 0.402776 val_accuracy: 0.390000\n",
      "iteration 0 / 5000: loss 7.182758\n",
      "iteration 100 / 5000: loss 3.792502\n",
      "iteration 200 / 5000: loss 3.553387\n",
      "iteration 300 / 5000: loss 3.110542\n",
      "iteration 400 / 5000: loss 2.934089\n",
      "iteration 500 / 5000: loss 2.977249\n",
      "iteration 600 / 5000: loss 3.008682\n",
      "iteration 700 / 5000: loss 2.863187\n",
      "iteration 800 / 5000: loss 2.876661\n",
      "iteration 900 / 5000: loss 2.933876\n",
      "iteration 1000 / 5000: loss 2.904450\n",
      "iteration 1100 / 5000: loss 2.609439\n",
      "iteration 1200 / 5000: loss 2.538429\n",
      "iteration 1300 / 5000: loss 2.870303\n",
      "iteration 1400 / 5000: loss 2.607766\n",
      "iteration 1500 / 5000: loss 2.615765\n",
      "iteration 1600 / 5000: loss 2.631439\n",
      "iteration 1700 / 5000: loss 2.754391\n",
      "iteration 1800 / 5000: loss 2.667276\n",
      "iteration 1900 / 5000: loss 2.637717\n",
      "iteration 2000 / 5000: loss 2.528657\n",
      "iteration 2100 / 5000: loss 2.528899\n",
      "iteration 2200 / 5000: loss 2.613694\n",
      "iteration 2300 / 5000: loss 2.651468\n",
      "iteration 2400 / 5000: loss 2.475072\n",
      "iteration 2500 / 5000: loss 2.542516\n",
      "iteration 2600 / 5000: loss 2.429088\n",
      "iteration 2700 / 5000: loss 2.560790\n",
      "iteration 2800 / 5000: loss 2.374145\n",
      "iteration 2900 / 5000: loss 2.577092\n",
      "iteration 3000 / 5000: loss 2.317013\n",
      "iteration 3100 / 5000: loss 2.377235\n",
      "iteration 3200 / 5000: loss 2.409451\n",
      "iteration 3300 / 5000: loss 2.394383\n",
      "iteration 3400 / 5000: loss 2.250793\n",
      "iteration 3500 / 5000: loss 2.302346\n",
      "iteration 3600 / 5000: loss 2.218330\n",
      "iteration 3700 / 5000: loss 2.354754\n",
      "iteration 3800 / 5000: loss 2.281644\n",
      "iteration 3900 / 5000: loss 2.311036\n",
      "iteration 4000 / 5000: loss 2.326238\n",
      "iteration 4100 / 5000: loss 2.336458\n",
      "iteration 4200 / 5000: loss 2.213371\n",
      "iteration 4300 / 5000: loss 2.353094\n",
      "iteration 4400 / 5000: loss 2.230778\n",
      "iteration 4500 / 5000: loss 2.324690\n",
      "iteration 4600 / 5000: loss 2.123628\n",
      "iteration 4700 / 5000: loss 2.235051\n",
      "iteration 4800 / 5000: loss 2.325550\n",
      "iteration 4900 / 5000: loss 2.154892\n",
      "lr 9.000000e-07 reg 3.000000e+01 train_accuracy: 0.406041 val_accuracy: 0.384000\n",
      "\n",
      "iteration 0 / 5000: loss 6.360572\n",
      "iteration 100 / 5000: loss 2.801455\n",
      "iteration 200 / 5000: loss 2.378760\n",
      "iteration 300 / 5000: loss 2.486292\n",
      "iteration 400 / 5000: loss 2.175007\n",
      "iteration 500 / 5000: loss 2.324057\n",
      "iteration 600 / 5000: loss 2.100465\n",
      "iteration 700 / 5000: loss 2.070962\n",
      "iteration 800 / 5000: loss 2.385727\n",
      "iteration 900 / 5000: loss 1.999519\n",
      "iteration 1000 / 5000: loss 1.928099\n",
      "iteration 1100 / 5000: loss 2.198480\n",
      "iteration 1200 / 5000: loss 1.895071\n",
      "iteration 1300 / 5000: loss 1.880732\n",
      "iteration 1400 / 5000: loss 1.919072\n",
      "iteration 1500 / 5000: loss 2.078927\n",
      "iteration 1600 / 5000: loss 1.869207\n",
      "iteration 1700 / 5000: loss 1.918191\n",
      "iteration 1800 / 5000: loss 1.880230\n",
      "iteration 1900 / 5000: loss 2.038415\n",
      "iteration 2000 / 5000: loss 1.828915\n",
      "iteration 2100 / 5000: loss 1.827664\n",
      "iteration 2200 / 5000: loss 1.904409\n",
      "iteration 2300 / 5000: loss 1.965851\n",
      "iteration 2400 / 5000: loss 1.817534\n",
      "iteration 2500 / 5000: loss 1.861780\n",
      "iteration 2600 / 5000: loss 1.921353\n",
      "iteration 2700 / 5000: loss 1.876054\n",
      "iteration 2800 / 5000: loss 1.762683\n",
      "iteration 2900 / 5000: loss 1.915407\n",
      "iteration 3000 / 5000: loss 1.739207\n",
      "iteration 3100 / 5000: loss 1.745664\n",
      "iteration 3200 / 5000: loss 1.890640\n",
      "iteration 3300 / 5000: loss 1.755473\n",
      "iteration 3400 / 5000: loss 1.692820\n",
      "iteration 3500 / 5000: loss 1.742119\n",
      "iteration 3600 / 5000: loss 1.762766\n",
      "iteration 3700 / 5000: loss 1.685488\n",
      "iteration 3800 / 5000: loss 1.933448\n",
      "iteration 3900 / 5000: loss 1.739378\n",
      "iteration 4000 / 5000: loss 1.752439\n",
      "iteration 4100 / 5000: loss 1.809102\n",
      "iteration 4200 / 5000: loss 1.618554\n",
      "iteration 4300 / 5000: loss 1.810125\n",
      "iteration 4400 / 5000: loss 1.732595\n",
      "iteration 4500 / 5000: loss 1.689327\n",
      "iteration 4600 / 5000: loss 1.836759\n",
      "iteration 4700 / 5000: loss 1.744632\n",
      "iteration 4800 / 5000: loss 1.751405\n",
      "iteration 4900 / 5000: loss 1.865202\n",
      "lr 1.000000e-06 reg 0.000000e+00 train_accuracy: 0.398714 val_accuracy: 0.391000\n",
      "iteration 0 / 5000: loss 5.794066\n",
      "iteration 100 / 5000: loss 3.302954\n",
      "iteration 200 / 5000: loss 2.967713\n",
      "iteration 300 / 5000: loss 2.603114\n",
      "iteration 400 / 5000: loss 2.774242\n",
      "iteration 500 / 5000: loss 2.542235\n",
      "iteration 600 / 5000: loss 2.354923\n",
      "iteration 700 / 5000: loss 2.428202\n",
      "iteration 800 / 5000: loss 2.389617\n",
      "iteration 900 / 5000: loss 2.237266\n",
      "iteration 1000 / 5000: loss 2.197362\n",
      "iteration 1100 / 5000: loss 2.253081\n",
      "iteration 1200 / 5000: loss 2.435489\n",
      "iteration 1300 / 5000: loss 2.222054\n",
      "iteration 1400 / 5000: loss 2.211893\n",
      "iteration 1500 / 5000: loss 2.255543\n",
      "iteration 1600 / 5000: loss 2.168218\n",
      "iteration 1700 / 5000: loss 2.103852\n",
      "iteration 1800 / 5000: loss 2.079239\n",
      "iteration 1900 / 5000: loss 2.191366\n",
      "iteration 2000 / 5000: loss 2.178138\n",
      "iteration 2100 / 5000: loss 2.247185\n",
      "iteration 2200 / 5000: loss 2.116843\n",
      "iteration 2300 / 5000: loss 2.173474\n",
      "iteration 2400 / 5000: loss 2.084446\n",
      "iteration 2500 / 5000: loss 1.931832\n",
      "iteration 2600 / 5000: loss 2.061922\n",
      "iteration 2700 / 5000: loss 2.125090\n",
      "iteration 2800 / 5000: loss 2.018508\n",
      "iteration 2900 / 5000: loss 2.162242\n",
      "iteration 3000 / 5000: loss 2.150096\n",
      "iteration 3100 / 5000: loss 2.006336\n",
      "iteration 3200 / 5000: loss 1.961450\n",
      "iteration 3300 / 5000: loss 2.120278\n",
      "iteration 3400 / 5000: loss 1.883099\n",
      "iteration 3500 / 5000: loss 2.016540\n",
      "iteration 3600 / 5000: loss 1.991573\n",
      "iteration 3700 / 5000: loss 1.963749\n",
      "iteration 3800 / 5000: loss 2.012203\n",
      "iteration 3900 / 5000: loss 2.026721\n",
      "iteration 4000 / 5000: loss 1.987842\n",
      "iteration 4100 / 5000: loss 1.993377\n",
      "iteration 4200 / 5000: loss 2.040095\n",
      "iteration 4300 / 5000: loss 1.947811\n",
      "iteration 4400 / 5000: loss 1.907277\n",
      "iteration 4500 / 5000: loss 1.960246\n",
      "iteration 4600 / 5000: loss 1.922833\n",
      "iteration 4700 / 5000: loss 1.870092\n",
      "iteration 4800 / 5000: loss 2.078417\n",
      "iteration 4900 / 5000: loss 1.959109\n",
      "lr 1.000000e-06 reg 1.000000e+01 train_accuracy: 0.404694 val_accuracy: 0.376000\n",
      "iteration 0 / 5000: loss 6.867621\n",
      "iteration 100 / 5000: loss 3.336973\n",
      "iteration 200 / 5000: loss 2.812020\n",
      "iteration 300 / 5000: loss 2.953249\n",
      "iteration 400 / 5000: loss 3.052467\n",
      "iteration 500 / 5000: loss 2.586129\n",
      "iteration 600 / 5000: loss 2.736371\n",
      "iteration 700 / 5000: loss 2.583538\n",
      "iteration 800 / 5000: loss 2.610087\n",
      "iteration 900 / 5000: loss 2.632078\n",
      "iteration 1000 / 5000: loss 2.328755\n",
      "iteration 1100 / 5000: loss 2.592812\n",
      "iteration 1200 / 5000: loss 2.247854\n",
      "iteration 1300 / 5000: loss 2.459121\n",
      "iteration 1400 / 5000: loss 2.477791\n",
      "iteration 1500 / 5000: loss 2.479667\n",
      "iteration 1600 / 5000: loss 2.546541\n",
      "iteration 1700 / 5000: loss 2.314790\n",
      "iteration 1800 / 5000: loss 2.289411\n",
      "iteration 1900 / 5000: loss 2.391152\n",
      "iteration 2000 / 5000: loss 2.288120\n",
      "iteration 2100 / 5000: loss 2.425690\n",
      "iteration 2200 / 5000: loss 2.226877\n",
      "iteration 2300 / 5000: loss 2.419409\n",
      "iteration 2400 / 5000: loss 2.296957\n",
      "iteration 2500 / 5000: loss 2.227254\n",
      "iteration 2600 / 5000: loss 2.188369\n",
      "iteration 2700 / 5000: loss 2.286898\n",
      "iteration 2800 / 5000: loss 2.169696\n",
      "iteration 2900 / 5000: loss 2.310491\n",
      "iteration 3000 / 5000: loss 2.301592\n",
      "iteration 3100 / 5000: loss 2.205675\n",
      "iteration 3200 / 5000: loss 2.186299\n",
      "iteration 3300 / 5000: loss 2.077411\n",
      "iteration 3400 / 5000: loss 2.383817\n",
      "iteration 3500 / 5000: loss 2.342136\n",
      "iteration 3600 / 5000: loss 2.007667\n",
      "iteration 3700 / 5000: loss 2.134019\n",
      "iteration 3800 / 5000: loss 2.183024\n",
      "iteration 3900 / 5000: loss 2.196769\n",
      "iteration 4000 / 5000: loss 2.040267\n",
      "iteration 4100 / 5000: loss 2.175368\n",
      "iteration 4200 / 5000: loss 2.200378\n",
      "iteration 4300 / 5000: loss 1.998870\n",
      "iteration 4400 / 5000: loss 2.254803\n",
      "iteration 4500 / 5000: loss 2.029928\n",
      "iteration 4600 / 5000: loss 2.135375\n",
      "iteration 4700 / 5000: loss 2.165441\n",
      "iteration 4800 / 5000: loss 2.023412\n",
      "iteration 4900 / 5000: loss 2.324362\n",
      "lr 1.000000e-06 reg 2.000000e+01 train_accuracy: 0.408184 val_accuracy: 0.386000\n",
      "iteration 0 / 5000: loss 7.770952\n",
      "iteration 100 / 5000: loss 3.398525\n",
      "iteration 200 / 5000: loss 3.403882\n",
      "iteration 300 / 5000: loss 3.206043\n",
      "iteration 400 / 5000: loss 3.148112\n",
      "iteration 500 / 5000: loss 2.945231\n",
      "iteration 600 / 5000: loss 2.798986\n",
      "iteration 700 / 5000: loss 2.850944\n",
      "iteration 800 / 5000: loss 2.917812\n",
      "iteration 900 / 5000: loss 2.764171\n",
      "iteration 1000 / 5000: loss 2.734237\n",
      "iteration 1100 / 5000: loss 2.733411\n",
      "iteration 1200 / 5000: loss 2.764819\n",
      "iteration 1300 / 5000: loss 2.932004\n",
      "iteration 1400 / 5000: loss 2.653553\n",
      "iteration 1500 / 5000: loss 2.610811\n",
      "iteration 1600 / 5000: loss 2.759274\n",
      "iteration 1700 / 5000: loss 2.553949\n",
      "iteration 1800 / 5000: loss 2.470591\n",
      "iteration 1900 / 5000: loss 2.604942\n",
      "iteration 2000 / 5000: loss 2.514193\n",
      "iteration 2100 / 5000: loss 2.631097\n",
      "iteration 2200 / 5000: loss 2.495612\n",
      "iteration 2300 / 5000: loss 2.591848\n",
      "iteration 2400 / 5000: loss 2.343353\n",
      "iteration 2500 / 5000: loss 2.404255\n",
      "iteration 2600 / 5000: loss 2.345186\n",
      "iteration 2700 / 5000: loss 2.495721\n",
      "iteration 2800 / 5000: loss 2.232771\n",
      "iteration 2900 / 5000: loss 2.229923\n",
      "iteration 3000 / 5000: loss 2.455878\n",
      "iteration 3100 / 5000: loss 2.459201\n",
      "iteration 3200 / 5000: loss 2.415375\n",
      "iteration 3300 / 5000: loss 2.260621\n",
      "iteration 3400 / 5000: loss 2.327576\n",
      "iteration 3500 / 5000: loss 2.182609\n",
      "iteration 3600 / 5000: loss 2.269909\n",
      "iteration 3700 / 5000: loss 2.302206\n",
      "iteration 3800 / 5000: loss 2.240540\n",
      "iteration 3900 / 5000: loss 2.461036\n",
      "iteration 4000 / 5000: loss 2.323066\n",
      "iteration 4100 / 5000: loss 2.159657\n",
      "iteration 4200 / 5000: loss 2.340117\n",
      "iteration 4300 / 5000: loss 2.071784\n",
      "iteration 4400 / 5000: loss 2.198875\n",
      "iteration 4500 / 5000: loss 2.257691\n",
      "iteration 4600 / 5000: loss 2.239220\n",
      "iteration 4700 / 5000: loss 2.248447\n",
      "iteration 4800 / 5000: loss 2.082845\n",
      "iteration 4900 / 5000: loss 2.110153\n",
      "lr 1.000000e-06 reg 3.000000e+01 train_accuracy: 0.410796 val_accuracy: 0.380000\n",
      "\n",
      "iteration 0 / 5000: loss 5.579101\n",
      "iteration 100 / 5000: loss 2.359505\n",
      "iteration 200 / 5000: loss 2.208382\n",
      "iteration 300 / 5000: loss 2.026929\n",
      "iteration 400 / 5000: loss 2.086660\n",
      "iteration 500 / 5000: loss 2.010847\n",
      "iteration 600 / 5000: loss 1.788584\n",
      "iteration 700 / 5000: loss 1.842434\n",
      "iteration 800 / 5000: loss 1.976112\n",
      "iteration 900 / 5000: loss 1.959415\n",
      "iteration 1000 / 5000: loss 1.820819\n",
      "iteration 1100 / 5000: loss 2.018902\n",
      "iteration 1200 / 5000: loss 1.702677\n",
      "iteration 1300 / 5000: loss 1.818799\n",
      "iteration 1400 / 5000: loss 1.751906\n",
      "iteration 1500 / 5000: loss 1.852733\n",
      "iteration 1600 / 5000: loss 1.780450\n",
      "iteration 1700 / 5000: loss 1.745322\n",
      "iteration 1800 / 5000: loss 1.738104\n",
      "iteration 1900 / 5000: loss 1.758823\n",
      "iteration 2000 / 5000: loss 1.757212\n",
      "iteration 2100 / 5000: loss 1.718626\n",
      "iteration 2200 / 5000: loss 1.801529\n",
      "iteration 2300 / 5000: loss 2.037068\n",
      "iteration 2400 / 5000: loss 1.626514\n",
      "iteration 2500 / 5000: loss 1.669359\n",
      "iteration 2600 / 5000: loss 1.845450\n",
      "iteration 2700 / 5000: loss 1.734267\n",
      "iteration 2800 / 5000: loss 1.800408\n",
      "iteration 2900 / 5000: loss 1.763581\n",
      "iteration 3000 / 5000: loss 1.825110\n",
      "iteration 3100 / 5000: loss 1.812320\n",
      "iteration 3200 / 5000: loss 1.878842\n",
      "iteration 3300 / 5000: loss 1.689380\n",
      "iteration 3400 / 5000: loss 1.773579\n",
      "iteration 3500 / 5000: loss 1.677485\n",
      "iteration 3600 / 5000: loss 1.660707\n",
      "iteration 3700 / 5000: loss 1.864938\n",
      "iteration 3800 / 5000: loss 1.636848\n",
      "iteration 3900 / 5000: loss 1.656711\n",
      "iteration 4000 / 5000: loss 1.811295\n",
      "iteration 4100 / 5000: loss 1.941059\n",
      "iteration 4200 / 5000: loss 1.873774\n",
      "iteration 4300 / 5000: loss 1.833638\n",
      "iteration 4400 / 5000: loss 1.757460\n",
      "iteration 4500 / 5000: loss 1.679926\n",
      "iteration 4600 / 5000: loss 1.723179\n",
      "iteration 4700 / 5000: loss 1.632027\n",
      "iteration 4800 / 5000: loss 1.710437\n",
      "iteration 4900 / 5000: loss 1.640681\n",
      "lr 3.000000e-06 reg 0.000000e+00 train_accuracy: 0.422204 val_accuracy: 0.367000\n",
      "iteration 0 / 5000: loss 6.024246\n",
      "iteration 100 / 5000: loss 2.718068\n",
      "iteration 200 / 5000: loss 2.508051\n",
      "iteration 300 / 5000: loss 2.306324\n",
      "iteration 400 / 5000: loss 2.162522\n",
      "iteration 500 / 5000: loss 2.364846\n",
      "iteration 600 / 5000: loss 2.191111\n",
      "iteration 700 / 5000: loss 2.134356\n",
      "iteration 800 / 5000: loss 2.073078\n",
      "iteration 900 / 5000: loss 2.093611\n",
      "iteration 1000 / 5000: loss 2.153518\n",
      "iteration 1100 / 5000: loss 2.183059\n",
      "iteration 1200 / 5000: loss 2.083309\n",
      "iteration 1300 / 5000: loss 2.086944\n",
      "iteration 1400 / 5000: loss 2.153596\n",
      "iteration 1500 / 5000: loss 1.873892\n",
      "iteration 1600 / 5000: loss 2.104566\n",
      "iteration 1700 / 5000: loss 2.178526\n",
      "iteration 1800 / 5000: loss 2.059268\n",
      "iteration 1900 / 5000: loss 1.967444\n",
      "iteration 2000 / 5000: loss 1.974806\n",
      "iteration 2100 / 5000: loss 2.049856\n",
      "iteration 2200 / 5000: loss 2.114878\n",
      "iteration 2300 / 5000: loss 2.007107\n",
      "iteration 2400 / 5000: loss 2.080178\n",
      "iteration 2500 / 5000: loss 1.838814\n",
      "iteration 2600 / 5000: loss 1.995751\n",
      "iteration 2700 / 5000: loss 1.853581\n",
      "iteration 2800 / 5000: loss 1.882043\n",
      "iteration 2900 / 5000: loss 1.859659\n",
      "iteration 3000 / 5000: loss 1.929152\n",
      "iteration 3100 / 5000: loss 1.839619\n",
      "iteration 3200 / 5000: loss 1.916338\n",
      "iteration 3300 / 5000: loss 1.969971\n",
      "iteration 3400 / 5000: loss 1.983189\n",
      "iteration 3500 / 5000: loss 1.976237\n",
      "iteration 3600 / 5000: loss 1.708267\n",
      "iteration 3700 / 5000: loss 1.822492\n",
      "iteration 3800 / 5000: loss 1.832224\n",
      "iteration 3900 / 5000: loss 1.791971\n",
      "iteration 4000 / 5000: loss 1.925640\n",
      "iteration 4100 / 5000: loss 1.837499\n",
      "iteration 4200 / 5000: loss 1.909463\n",
      "iteration 4300 / 5000: loss 1.997981\n",
      "iteration 4400 / 5000: loss 1.810655\n",
      "iteration 4500 / 5000: loss 1.970834\n",
      "iteration 4600 / 5000: loss 1.820844\n",
      "iteration 4700 / 5000: loss 1.780043\n",
      "iteration 4800 / 5000: loss 1.961121\n",
      "iteration 4900 / 5000: loss 1.853914\n",
      "lr 3.000000e-06 reg 1.000000e+01 train_accuracy: 0.429755 val_accuracy: 0.393000\n",
      "iteration 0 / 5000: loss 6.454193\n",
      "iteration 100 / 5000: loss 2.909984\n",
      "iteration 200 / 5000: loss 2.672255\n",
      "iteration 300 / 5000: loss 2.738385\n",
      "iteration 400 / 5000: loss 2.562657\n",
      "iteration 500 / 5000: loss 2.381145\n",
      "iteration 600 / 5000: loss 2.503709\n",
      "iteration 700 / 5000: loss 2.514114\n",
      "iteration 800 / 5000: loss 2.258335\n",
      "iteration 900 / 5000: loss 2.393176\n",
      "iteration 1000 / 5000: loss 2.330656\n",
      "iteration 1100 / 5000: loss 2.092920\n",
      "iteration 1200 / 5000: loss 2.146216\n",
      "iteration 1300 / 5000: loss 2.261024\n",
      "iteration 1400 / 5000: loss 2.259106\n",
      "iteration 1500 / 5000: loss 2.086675\n",
      "iteration 1600 / 5000: loss 2.116290\n",
      "iteration 1700 / 5000: loss 2.060138\n",
      "iteration 1800 / 5000: loss 1.912304\n",
      "iteration 1900 / 5000: loss 2.120896\n",
      "iteration 2000 / 5000: loss 2.202104\n",
      "iteration 2100 / 5000: loss 2.052783\n",
      "iteration 2200 / 5000: loss 1.953162\n",
      "iteration 2300 / 5000: loss 2.148624\n",
      "iteration 2400 / 5000: loss 1.929310\n",
      "iteration 2500 / 5000: loss 2.081393\n",
      "iteration 2600 / 5000: loss 1.904832\n",
      "iteration 2700 / 5000: loss 2.004426\n",
      "iteration 2800 / 5000: loss 2.041455\n",
      "iteration 2900 / 5000: loss 1.955354\n",
      "iteration 3000 / 5000: loss 1.959096\n",
      "iteration 3100 / 5000: loss 1.891320\n",
      "iteration 3200 / 5000: loss 1.980676\n",
      "iteration 3300 / 5000: loss 1.754644\n",
      "iteration 3400 / 5000: loss 1.889816\n",
      "iteration 3500 / 5000: loss 1.970623\n",
      "iteration 3600 / 5000: loss 1.954540\n",
      "iteration 3700 / 5000: loss 1.890593\n",
      "iteration 3800 / 5000: loss 1.851183\n",
      "iteration 3900 / 5000: loss 1.975085\n",
      "iteration 4000 / 5000: loss 1.898329\n",
      "iteration 4100 / 5000: loss 1.864786\n",
      "iteration 4200 / 5000: loss 1.980209\n",
      "iteration 4300 / 5000: loss 1.842804\n",
      "iteration 4400 / 5000: loss 1.825380\n",
      "iteration 4500 / 5000: loss 1.830144\n",
      "iteration 4600 / 5000: loss 1.808343\n",
      "iteration 4700 / 5000: loss 1.860571\n",
      "iteration 4800 / 5000: loss 1.859681\n",
      "iteration 4900 / 5000: loss 1.855746\n",
      "lr 3.000000e-06 reg 2.000000e+01 train_accuracy: 0.436898 val_accuracy: 0.398000\n",
      "iteration 0 / 5000: loss 6.836626\n",
      "iteration 100 / 5000: loss 3.393198\n",
      "iteration 200 / 5000: loss 2.786902\n",
      "iteration 300 / 5000: loss 2.804638\n",
      "iteration 400 / 5000: loss 2.622994\n",
      "iteration 500 / 5000: loss 2.546162\n",
      "iteration 600 / 5000: loss 2.536640\n",
      "iteration 700 / 5000: loss 2.554380\n",
      "iteration 800 / 5000: loss 2.301646\n",
      "iteration 900 / 5000: loss 2.558034\n",
      "iteration 1000 / 5000: loss 2.347489\n",
      "iteration 1100 / 5000: loss 2.254336\n",
      "iteration 1200 / 5000: loss 2.267181\n",
      "iteration 1300 / 5000: loss 2.250335\n",
      "iteration 1400 / 5000: loss 2.228238\n",
      "iteration 1500 / 5000: loss 2.103388\n",
      "iteration 1600 / 5000: loss 2.181427\n",
      "iteration 1700 / 5000: loss 2.216838\n",
      "iteration 1800 / 5000: loss 2.138330\n",
      "iteration 1900 / 5000: loss 2.262807\n",
      "iteration 2000 / 5000: loss 2.094658\n",
      "iteration 2100 / 5000: loss 2.129619\n",
      "iteration 2200 / 5000: loss 1.982128\n",
      "iteration 2300 / 5000: loss 2.151801\n",
      "iteration 2400 / 5000: loss 1.999050\n",
      "iteration 2500 / 5000: loss 2.065991\n",
      "iteration 2600 / 5000: loss 1.953773\n",
      "iteration 2700 / 5000: loss 2.160250\n",
      "iteration 2800 / 5000: loss 1.973993\n",
      "iteration 2900 / 5000: loss 2.075466\n",
      "iteration 3000 / 5000: loss 2.019466\n",
      "iteration 3100 / 5000: loss 1.900529\n",
      "iteration 3200 / 5000: loss 1.857706\n",
      "iteration 3300 / 5000: loss 1.992052\n",
      "iteration 3400 / 5000: loss 1.931678\n",
      "iteration 3500 / 5000: loss 2.075879\n",
      "iteration 3600 / 5000: loss 1.992027\n",
      "iteration 3700 / 5000: loss 1.742050\n",
      "iteration 3800 / 5000: loss 1.927766\n",
      "iteration 3900 / 5000: loss 1.917718\n",
      "iteration 4000 / 5000: loss 1.880454\n",
      "iteration 4100 / 5000: loss 1.875124\n",
      "iteration 4200 / 5000: loss 1.913980\n",
      "iteration 4300 / 5000: loss 1.834700\n",
      "iteration 4400 / 5000: loss 1.886777\n",
      "iteration 4500 / 5000: loss 1.818882\n",
      "iteration 4600 / 5000: loss 1.786406\n",
      "iteration 4700 / 5000: loss 1.864157\n",
      "iteration 4800 / 5000: loss 1.895410\n",
      "iteration 4900 / 5000: loss 1.778008\n",
      "lr 3.000000e-06 reg 3.000000e+01 train_accuracy: 0.428061 val_accuracy: 0.409000\n",
      "\n",
      "iteration 0 / 5000: loss 5.255086\n",
      "iteration 100 / 5000: loss 3.432331\n",
      "iteration 200 / 5000: loss 2.809733\n",
      "iteration 300 / 5000: loss 2.470584\n",
      "iteration 400 / 5000: loss 2.594360\n",
      "iteration 500 / 5000: loss 2.224085\n",
      "iteration 600 / 5000: loss 3.102964\n",
      "iteration 700 / 5000: loss 2.932607\n",
      "iteration 800 / 5000: loss 2.062852\n",
      "iteration 900 / 5000: loss 2.803011\n",
      "iteration 1000 / 5000: loss 2.098896\n",
      "iteration 1100 / 5000: loss 2.742945\n",
      "iteration 1200 / 5000: loss 2.879248\n",
      "iteration 1300 / 5000: loss 2.703534\n",
      "iteration 1400 / 5000: loss 2.921872\n",
      "iteration 1500 / 5000: loss 2.221160\n",
      "iteration 1600 / 5000: loss 1.880621\n",
      "iteration 1700 / 5000: loss 2.541900\n",
      "iteration 1800 / 5000: loss 2.204998\n",
      "iteration 1900 / 5000: loss 2.794943\n",
      "iteration 2000 / 5000: loss 2.264381\n",
      "iteration 2100 / 5000: loss 2.747814\n",
      "iteration 2200 / 5000: loss 1.963404\n",
      "iteration 2300 / 5000: loss 2.331548\n",
      "iteration 2400 / 5000: loss 2.598159\n",
      "iteration 2500 / 5000: loss 2.202516\n",
      "iteration 2600 / 5000: loss 2.712103\n",
      "iteration 2700 / 5000: loss 3.228488\n",
      "iteration 2800 / 5000: loss 2.238277\n",
      "iteration 2900 / 5000: loss 2.091414\n",
      "iteration 3000 / 5000: loss 1.916260\n",
      "iteration 3100 / 5000: loss 2.321303\n",
      "iteration 3200 / 5000: loss 2.366662\n",
      "iteration 3300 / 5000: loss 2.923833\n",
      "iteration 3400 / 5000: loss 2.371701\n",
      "iteration 3500 / 5000: loss 2.131748\n",
      "iteration 3600 / 5000: loss 1.853840\n",
      "iteration 3700 / 5000: loss 2.649496\n",
      "iteration 3800 / 5000: loss 2.136003\n",
      "iteration 3900 / 5000: loss 2.925092\n",
      "iteration 4000 / 5000: loss 2.518851\n",
      "iteration 4100 / 5000: loss 3.085701\n",
      "iteration 4200 / 5000: loss 2.074761\n",
      "iteration 4300 / 5000: loss 1.854290\n",
      "iteration 4400 / 5000: loss 2.610373\n",
      "iteration 4500 / 5000: loss 2.892520\n",
      "iteration 4600 / 5000: loss 3.357470\n",
      "iteration 4700 / 5000: loss 2.098251\n",
      "iteration 4800 / 5000: loss 1.924147\n",
      "iteration 4900 / 5000: loss 2.069370\n",
      "lr 9.000000e-06 reg 0.000000e+00 train_accuracy: 0.386388 val_accuracy: 0.342000\n",
      "iteration 0 / 5000: loss 5.041096\n",
      "iteration 100 / 5000: loss 2.800540\n",
      "iteration 200 / 5000: loss 2.556524\n",
      "iteration 300 / 5000: loss 2.770911\n",
      "iteration 400 / 5000: loss 2.359258\n",
      "iteration 500 / 5000: loss 2.867574\n",
      "iteration 600 / 5000: loss 2.768230\n",
      "iteration 700 / 5000: loss 2.568220\n",
      "iteration 800 / 5000: loss 2.634586\n",
      "iteration 900 / 5000: loss 2.584697\n",
      "iteration 1000 / 5000: loss 2.048333\n",
      "iteration 1100 / 5000: loss 2.578358\n",
      "iteration 1200 / 5000: loss 2.980895\n",
      "iteration 1300 / 5000: loss 2.487833\n",
      "iteration 1400 / 5000: loss 3.699052\n",
      "iteration 1500 / 5000: loss 2.273859\n",
      "iteration 1600 / 5000: loss 2.298473\n",
      "iteration 1700 / 5000: loss 2.592677\n",
      "iteration 1800 / 5000: loss 2.071713\n",
      "iteration 1900 / 5000: loss 2.850635\n",
      "iteration 2000 / 5000: loss 2.648414\n",
      "iteration 2100 / 5000: loss 2.437301\n",
      "iteration 2200 / 5000: loss 2.251392\n",
      "iteration 2300 / 5000: loss 2.559390\n",
      "iteration 2400 / 5000: loss 2.625447\n",
      "iteration 2500 / 5000: loss 2.468310\n",
      "iteration 2600 / 5000: loss 3.030068\n",
      "iteration 2700 / 5000: loss 2.256557\n",
      "iteration 2800 / 5000: loss 2.860518\n",
      "iteration 2900 / 5000: loss 1.830774\n",
      "iteration 3000 / 5000: loss 2.307239\n",
      "iteration 3100 / 5000: loss 2.386559\n",
      "iteration 3200 / 5000: loss 2.853774\n",
      "iteration 3300 / 5000: loss 3.345607\n",
      "iteration 3400 / 5000: loss 3.216684\n",
      "iteration 3500 / 5000: loss 2.375279\n",
      "iteration 3600 / 5000: loss 3.041149\n",
      "iteration 3700 / 5000: loss 2.478628\n",
      "iteration 3800 / 5000: loss 2.362251\n",
      "iteration 3900 / 5000: loss 2.682318\n",
      "iteration 4000 / 5000: loss 2.862485\n",
      "iteration 4100 / 5000: loss 2.578177\n",
      "iteration 4200 / 5000: loss 2.991762\n",
      "iteration 4300 / 5000: loss 2.256345\n",
      "iteration 4400 / 5000: loss 2.010826\n",
      "iteration 4500 / 5000: loss 4.033802\n",
      "iteration 4600 / 5000: loss 2.691317\n",
      "iteration 4700 / 5000: loss 2.500369\n",
      "iteration 4800 / 5000: loss 3.278342\n",
      "iteration 4900 / 5000: loss 2.181368\n",
      "lr 9.000000e-06 reg 1.000000e+01 train_accuracy: 0.321143 val_accuracy: 0.290000\n",
      "iteration 0 / 5000: loss 5.336677\n",
      "iteration 100 / 5000: loss 2.909316\n",
      "iteration 200 / 5000: loss 3.006543\n",
      "iteration 300 / 5000: loss 2.986438\n",
      "iteration 400 / 5000: loss 3.291382\n",
      "iteration 500 / 5000: loss 2.492118\n",
      "iteration 600 / 5000: loss 2.676696\n",
      "iteration 700 / 5000: loss 2.733394\n",
      "iteration 800 / 5000: loss 3.737363\n",
      "iteration 900 / 5000: loss 2.818817\n",
      "iteration 1000 / 5000: loss 2.597523\n",
      "iteration 1100 / 5000: loss 3.773572\n",
      "iteration 1200 / 5000: loss 3.155225\n",
      "iteration 1300 / 5000: loss 2.593658\n",
      "iteration 1400 / 5000: loss 2.806002\n",
      "iteration 1500 / 5000: loss 2.546008\n",
      "iteration 1600 / 5000: loss 2.546144\n",
      "iteration 1700 / 5000: loss 2.541680\n",
      "iteration 1800 / 5000: loss 2.767312\n",
      "iteration 1900 / 5000: loss 2.449708\n",
      "iteration 2000 / 5000: loss 2.606908\n",
      "iteration 2100 / 5000: loss 2.371782\n",
      "iteration 2200 / 5000: loss 2.969236\n",
      "iteration 2300 / 5000: loss 2.818390\n",
      "iteration 2400 / 5000: loss 3.377813\n",
      "iteration 2500 / 5000: loss 3.328898\n",
      "iteration 2600 / 5000: loss 3.280727\n",
      "iteration 2700 / 5000: loss 2.192152\n",
      "iteration 2800 / 5000: loss 2.159228\n",
      "iteration 2900 / 5000: loss 2.574870\n",
      "iteration 3000 / 5000: loss 2.077236\n",
      "iteration 3100 / 5000: loss 2.502681\n",
      "iteration 3200 / 5000: loss 2.328942\n",
      "iteration 3300 / 5000: loss 2.238873\n",
      "iteration 3400 / 5000: loss 2.266940\n",
      "iteration 3500 / 5000: loss 2.233484\n",
      "iteration 3600 / 5000: loss 2.454057\n",
      "iteration 3700 / 5000: loss 2.455141\n",
      "iteration 3800 / 5000: loss 2.906823\n",
      "iteration 3900 / 5000: loss 2.468250\n",
      "iteration 4000 / 5000: loss 2.312085\n",
      "iteration 4100 / 5000: loss 2.311764\n",
      "iteration 4200 / 5000: loss 2.309835\n",
      "iteration 4300 / 5000: loss 2.217428\n",
      "iteration 4400 / 5000: loss 2.897619\n",
      "iteration 4500 / 5000: loss 2.642046\n",
      "iteration 4600 / 5000: loss 2.519733\n",
      "iteration 4700 / 5000: loss 2.280202\n",
      "iteration 4800 / 5000: loss 2.273049\n",
      "iteration 4900 / 5000: loss 2.614350\n",
      "lr 9.000000e-06 reg 2.000000e+01 train_accuracy: 0.328653 val_accuracy: 0.322000\n",
      "iteration 0 / 5000: loss 6.038862\n",
      "iteration 100 / 5000: loss 3.186071\n",
      "iteration 200 / 5000: loss 3.861151\n",
      "iteration 300 / 5000: loss 3.310606\n",
      "iteration 400 / 5000: loss 2.618484\n",
      "iteration 500 / 5000: loss 2.793410\n",
      "iteration 600 / 5000: loss 2.905470\n",
      "iteration 700 / 5000: loss 2.912736\n",
      "iteration 800 / 5000: loss 2.707339\n",
      "iteration 900 / 5000: loss 2.913764\n",
      "iteration 1000 / 5000: loss 2.654258\n",
      "iteration 1100 / 5000: loss 2.614473\n",
      "iteration 1200 / 5000: loss 2.320458\n",
      "iteration 1300 / 5000: loss 2.589366\n",
      "iteration 1400 / 5000: loss 2.427446\n",
      "iteration 1500 / 5000: loss 4.614443\n",
      "iteration 1600 / 5000: loss 2.104315\n",
      "iteration 1700 / 5000: loss 2.319863\n",
      "iteration 1800 / 5000: loss 2.186571\n",
      "iteration 1900 / 5000: loss 2.517079\n",
      "iteration 2000 / 5000: loss 2.267435\n",
      "iteration 2100 / 5000: loss 2.315993\n",
      "iteration 2200 / 5000: loss 2.145067\n",
      "iteration 2300 / 5000: loss 2.797983\n",
      "iteration 2400 / 5000: loss 2.298326\n",
      "iteration 2500 / 5000: loss 2.485202\n",
      "iteration 2600 / 5000: loss 2.492742\n",
      "iteration 2700 / 5000: loss 2.337195\n",
      "iteration 2800 / 5000: loss 3.212484\n",
      "iteration 2900 / 5000: loss 2.267784\n",
      "iteration 3000 / 5000: loss 3.203229\n",
      "iteration 3100 / 5000: loss 2.279870\n",
      "iteration 3200 / 5000: loss 3.388173\n",
      "iteration 3300 / 5000: loss 2.151430\n",
      "iteration 3400 / 5000: loss 2.594671\n",
      "iteration 3500 / 5000: loss 3.235025\n",
      "iteration 3600 / 5000: loss 2.345452\n",
      "iteration 3700 / 5000: loss 2.257420\n",
      "iteration 3800 / 5000: loss 3.052169\n",
      "iteration 3900 / 5000: loss 2.541563\n",
      "iteration 4000 / 5000: loss 2.170400\n",
      "iteration 4100 / 5000: loss 2.670277\n",
      "iteration 4200 / 5000: loss 2.162022\n",
      "iteration 4300 / 5000: loss 2.796344\n",
      "iteration 4400 / 5000: loss 2.667445\n",
      "iteration 4500 / 5000: loss 3.126176\n",
      "iteration 4600 / 5000: loss 3.276190\n",
      "iteration 4700 / 5000: loss 2.018075\n",
      "iteration 4800 / 5000: loss 2.004788\n",
      "iteration 4900 / 5000: loss 3.537694\n",
      "lr 9.000000e-06 reg 3.000000e+01 train_accuracy: 0.288082 val_accuracy: 0.285000\n",
      "\n",
      "iteration 0 / 5000: loss 5.016661\n",
      "iteration 100 / 5000: loss 4.155857\n",
      "iteration 200 / 5000: loss 2.946559\n",
      "iteration 300 / 5000: loss 3.785657\n",
      "iteration 400 / 5000: loss 3.222257\n",
      "iteration 500 / 5000: loss 2.546774\n",
      "iteration 600 / 5000: loss 2.656769\n",
      "iteration 700 / 5000: loss 2.939526\n",
      "iteration 800 / 5000: loss 3.634860\n",
      "iteration 900 / 5000: loss 3.478608\n",
      "iteration 1000 / 5000: loss 2.404828\n",
      "iteration 1100 / 5000: loss 2.169717\n",
      "iteration 1200 / 5000: loss 2.214158\n",
      "iteration 1300 / 5000: loss 2.408467\n",
      "iteration 1400 / 5000: loss 2.599862\n",
      "iteration 1500 / 5000: loss 2.997066\n",
      "iteration 1600 / 5000: loss 2.405257\n",
      "iteration 1700 / 5000: loss 2.039231\n",
      "iteration 1800 / 5000: loss 2.854190\n",
      "iteration 1900 / 5000: loss 2.781875\n",
      "iteration 2000 / 5000: loss 3.430944\n",
      "iteration 2100 / 5000: loss 2.388086\n",
      "iteration 2200 / 5000: loss 2.351513\n",
      "iteration 2300 / 5000: loss 2.550103\n",
      "iteration 2400 / 5000: loss 2.116575\n",
      "iteration 2500 / 5000: loss 2.365894\n",
      "iteration 2600 / 5000: loss 2.380938\n",
      "iteration 2700 / 5000: loss 3.008602\n",
      "iteration 2800 / 5000: loss 3.658783\n",
      "iteration 2900 / 5000: loss 2.476036\n",
      "iteration 3000 / 5000: loss 2.732098\n",
      "iteration 3100 / 5000: loss 2.292632\n",
      "iteration 3200 / 5000: loss 2.865500\n",
      "iteration 3300 / 5000: loss 2.334434\n",
      "iteration 3400 / 5000: loss 2.843120\n",
      "iteration 3500 / 5000: loss 2.581096\n",
      "iteration 3600 / 5000: loss 2.752521\n",
      "iteration 3700 / 5000: loss 2.494821\n",
      "iteration 3800 / 5000: loss 2.965576\n",
      "iteration 3900 / 5000: loss 3.565420\n",
      "iteration 4000 / 5000: loss 2.372658\n",
      "iteration 4100 / 5000: loss 2.682700\n",
      "iteration 4200 / 5000: loss 2.688395\n",
      "iteration 4300 / 5000: loss 2.575862\n",
      "iteration 4400 / 5000: loss 3.432930\n",
      "iteration 4500 / 5000: loss 2.575548\n",
      "iteration 4600 / 5000: loss 3.153729\n",
      "iteration 4700 / 5000: loss 2.630418\n",
      "iteration 4800 / 5000: loss 2.795335\n",
      "iteration 4900 / 5000: loss 2.188269\n",
      "lr 1.000000e-05 reg 0.000000e+00 train_accuracy: 0.366286 val_accuracy: 0.341000\n",
      "iteration 0 / 5000: loss 5.791298\n",
      "iteration 100 / 5000: loss 3.440958\n",
      "iteration 200 / 5000: loss 3.779345\n",
      "iteration 300 / 5000: loss 3.485605\n",
      "iteration 400 / 5000: loss 2.726658\n",
      "iteration 500 / 5000: loss 3.757652\n",
      "iteration 600 / 5000: loss 3.530423\n",
      "iteration 700 / 5000: loss 3.199600\n",
      "iteration 800 / 5000: loss 3.027865\n",
      "iteration 900 / 5000: loss 3.600032\n",
      "iteration 1000 / 5000: loss 2.710552\n",
      "iteration 1100 / 5000: loss 3.035919\n",
      "iteration 1200 / 5000: loss 3.225897\n",
      "iteration 1300 / 5000: loss 2.770037\n",
      "iteration 1400 / 5000: loss 2.874390\n",
      "iteration 1500 / 5000: loss 2.579563\n",
      "iteration 1600 / 5000: loss 3.405138\n",
      "iteration 1700 / 5000: loss 2.599846\n",
      "iteration 1800 / 5000: loss 2.917686\n",
      "iteration 1900 / 5000: loss 2.362158\n",
      "iteration 2000 / 5000: loss 2.796984\n",
      "iteration 2100 / 5000: loss 2.969405\n",
      "iteration 2200 / 5000: loss 3.833965\n",
      "iteration 2300 / 5000: loss 2.860082\n",
      "iteration 2400 / 5000: loss 2.382388\n",
      "iteration 2500 / 5000: loss 2.207060\n",
      "iteration 2600 / 5000: loss 2.311406\n",
      "iteration 2700 / 5000: loss 2.625075\n",
      "iteration 2800 / 5000: loss 3.179251\n",
      "iteration 2900 / 5000: loss 2.623061\n",
      "iteration 3000 / 5000: loss 3.111991\n",
      "iteration 3100 / 5000: loss 3.450343\n",
      "iteration 3200 / 5000: loss 2.250156\n",
      "iteration 3300 / 5000: loss 2.404153\n",
      "iteration 3400 / 5000: loss 2.667039\n",
      "iteration 3500 / 5000: loss 3.376922\n",
      "iteration 3600 / 5000: loss 2.735514\n",
      "iteration 3700 / 5000: loss 2.384534\n",
      "iteration 3800 / 5000: loss 2.795430\n",
      "iteration 3900 / 5000: loss 2.784857\n",
      "iteration 4000 / 5000: loss 2.273599\n",
      "iteration 4100 / 5000: loss 2.901629\n",
      "iteration 4200 / 5000: loss 3.489448\n",
      "iteration 4300 / 5000: loss 2.665009\n",
      "iteration 4400 / 5000: loss 2.991187\n",
      "iteration 4500 / 5000: loss 3.092421\n",
      "iteration 4600 / 5000: loss 2.336501\n",
      "iteration 4700 / 5000: loss 4.223221\n",
      "iteration 4800 / 5000: loss 2.948641\n",
      "iteration 4900 / 5000: loss 2.702853\n",
      "lr 1.000000e-05 reg 1.000000e+01 train_accuracy: 0.384082 val_accuracy: 0.332000\n",
      "iteration 0 / 5000: loss 6.163628\n",
      "iteration 100 / 5000: loss 3.276208\n",
      "iteration 200 / 5000: loss 3.456445\n",
      "iteration 300 / 5000: loss 3.089058\n",
      "iteration 400 / 5000: loss 4.906455\n",
      "iteration 500 / 5000: loss 3.190141\n",
      "iteration 600 / 5000: loss 2.986315\n",
      "iteration 700 / 5000: loss 2.848284\n",
      "iteration 800 / 5000: loss 3.214327\n",
      "iteration 900 / 5000: loss 3.431559\n",
      "iteration 1000 / 5000: loss 3.069665\n",
      "iteration 1100 / 5000: loss 3.659971\n",
      "iteration 1200 / 5000: loss 2.858215\n",
      "iteration 1300 / 5000: loss 2.454452\n",
      "iteration 1400 / 5000: loss 2.422976\n",
      "iteration 1500 / 5000: loss 2.621479\n",
      "iteration 1600 / 5000: loss 2.889756\n",
      "iteration 1700 / 5000: loss 2.714707\n",
      "iteration 1800 / 5000: loss 2.419300\n",
      "iteration 1900 / 5000: loss 2.646918\n",
      "iteration 2000 / 5000: loss 2.842511\n",
      "iteration 2100 / 5000: loss 2.649382\n",
      "iteration 2200 / 5000: loss 2.772975\n",
      "iteration 2300 / 5000: loss 3.033201\n",
      "iteration 2400 / 5000: loss 2.978408\n",
      "iteration 2500 / 5000: loss 2.144853\n",
      "iteration 2600 / 5000: loss 2.181829\n",
      "iteration 2700 / 5000: loss 3.240294\n",
      "iteration 2800 / 5000: loss 3.216088\n",
      "iteration 2900 / 5000: loss 2.352159\n",
      "iteration 3000 / 5000: loss 3.184868\n",
      "iteration 3100 / 5000: loss 2.375062\n",
      "iteration 3200 / 5000: loss 2.853425\n",
      "iteration 3300 / 5000: loss 2.670030\n",
      "iteration 3400 / 5000: loss 4.111603\n",
      "iteration 3500 / 5000: loss 2.185689\n",
      "iteration 3600 / 5000: loss 2.604840\n",
      "iteration 3700 / 5000: loss 2.285340\n",
      "iteration 3800 / 5000: loss 2.830536\n",
      "iteration 3900 / 5000: loss 2.833625\n",
      "iteration 4000 / 5000: loss 2.588755\n",
      "iteration 4100 / 5000: loss 2.353959\n",
      "iteration 4200 / 5000: loss 2.450299\n",
      "iteration 4300 / 5000: loss 2.993622\n",
      "iteration 4400 / 5000: loss 2.641635\n",
      "iteration 4500 / 5000: loss 2.460381\n",
      "iteration 4600 / 5000: loss 2.660724\n",
      "iteration 4700 / 5000: loss 2.724993\n",
      "iteration 4800 / 5000: loss 2.181915\n",
      "iteration 4900 / 5000: loss 2.226334\n",
      "lr 1.000000e-05 reg 2.000000e+01 train_accuracy: 0.289143 val_accuracy: 0.273000\n",
      "iteration 0 / 5000: loss 7.159946\n",
      "iteration 100 / 5000: loss 3.802031\n",
      "iteration 200 / 5000: loss 4.027112\n",
      "iteration 300 / 5000: loss 3.116680\n",
      "iteration 400 / 5000: loss 2.850288\n",
      "iteration 500 / 5000: loss 3.738379\n",
      "iteration 600 / 5000: loss 4.282948\n",
      "iteration 700 / 5000: loss 3.251790\n",
      "iteration 800 / 5000: loss 2.750396\n",
      "iteration 900 / 5000: loss 3.522368\n",
      "iteration 1000 / 5000: loss 2.664103\n",
      "iteration 1100 / 5000: loss 2.973611\n",
      "iteration 1200 / 5000: loss 3.760410\n",
      "iteration 1300 / 5000: loss 2.495893\n",
      "iteration 1400 / 5000: loss 2.457744\n",
      "iteration 1500 / 5000: loss 2.846360\n",
      "iteration 1600 / 5000: loss 2.493234\n",
      "iteration 1700 / 5000: loss 3.948903\n",
      "iteration 1800 / 5000: loss 3.252895\n",
      "iteration 1900 / 5000: loss 2.092049\n",
      "iteration 2000 / 5000: loss 2.317510\n",
      "iteration 2100 / 5000: loss 3.078219\n",
      "iteration 2200 / 5000: loss 3.286067\n",
      "iteration 2300 / 5000: loss 2.532288\n",
      "iteration 2400 / 5000: loss 2.613338\n",
      "iteration 2500 / 5000: loss 3.252645\n",
      "iteration 2600 / 5000: loss 3.567793\n",
      "iteration 2700 / 5000: loss 2.720252\n",
      "iteration 2800 / 5000: loss 2.831134\n",
      "iteration 2900 / 5000: loss 2.256788\n",
      "iteration 3000 / 5000: loss 2.643043\n",
      "iteration 3100 / 5000: loss 2.337396\n",
      "iteration 3200 / 5000: loss 3.057481\n",
      "iteration 3300 / 5000: loss 2.619867\n",
      "iteration 3400 / 5000: loss 3.304847\n",
      "iteration 3500 / 5000: loss 2.786456\n",
      "iteration 3600 / 5000: loss 2.946485\n",
      "iteration 3700 / 5000: loss 2.394184\n",
      "iteration 3800 / 5000: loss 2.485016\n",
      "iteration 3900 / 5000: loss 3.760162\n",
      "iteration 4000 / 5000: loss 4.052855\n",
      "iteration 4100 / 5000: loss 3.412868\n",
      "iteration 4200 / 5000: loss 2.541334\n",
      "iteration 4300 / 5000: loss 2.530453\n",
      "iteration 4400 / 5000: loss 3.262062\n",
      "iteration 4500 / 5000: loss 2.287390\n",
      "iteration 4600 / 5000: loss 2.273288\n",
      "iteration 4700 / 5000: loss 2.396527\n",
      "iteration 4800 / 5000: loss 3.260886\n",
      "iteration 4900 / 5000: loss 3.702511\n",
      "lr 1.000000e-05 reg 3.000000e+01 train_accuracy: 0.301653 val_accuracy: 0.291000\n",
      "\n",
      "lr 9.000000e-07 reg 0.000000e+00 train_accuracy: 0.395347 val_accuracy: 0.354000\n",
      "lr 9.000000e-07 reg 1.000000e+01 train_accuracy: 0.394551 val_accuracy: 0.355000\n",
      "lr 9.000000e-07 reg 2.000000e+01 train_accuracy: 0.402776 val_accuracy: 0.390000\n",
      "lr 9.000000e-07 reg 3.000000e+01 train_accuracy: 0.406041 val_accuracy: 0.384000\n",
      "lr 1.000000e-06 reg 0.000000e+00 train_accuracy: 0.398714 val_accuracy: 0.391000\n",
      "lr 1.000000e-06 reg 1.000000e+01 train_accuracy: 0.404694 val_accuracy: 0.376000\n",
      "lr 1.000000e-06 reg 2.000000e+01 train_accuracy: 0.408184 val_accuracy: 0.386000\n",
      "lr 1.000000e-06 reg 3.000000e+01 train_accuracy: 0.410796 val_accuracy: 0.380000\n",
      "lr 3.000000e-06 reg 0.000000e+00 train_accuracy: 0.422204 val_accuracy: 0.367000\n",
      "lr 3.000000e-06 reg 1.000000e+01 train_accuracy: 0.429755 val_accuracy: 0.393000\n",
      "lr 3.000000e-06 reg 2.000000e+01 train_accuracy: 0.436898 val_accuracy: 0.398000\n",
      "lr 3.000000e-06 reg 3.000000e+01 train_accuracy: 0.428061 val_accuracy: 0.409000\n",
      "lr 9.000000e-06 reg 0.000000e+00 train_accuracy: 0.386388 val_accuracy: 0.342000\n",
      "lr 9.000000e-06 reg 1.000000e+01 train_accuracy: 0.321143 val_accuracy: 0.290000\n",
      "lr 9.000000e-06 reg 2.000000e+01 train_accuracy: 0.328653 val_accuracy: 0.322000\n",
      "lr 9.000000e-06 reg 3.000000e+01 train_accuracy: 0.288082 val_accuracy: 0.285000\n",
      "lr 1.000000e-05 reg 0.000000e+00 train_accuracy: 0.366286 val_accuracy: 0.341000\n",
      "lr 1.000000e-05 reg 1.000000e+01 train_accuracy: 0.384082 val_accuracy: 0.332000\n",
      "lr 1.000000e-05 reg 2.000000e+01 train_accuracy: 0.289143 val_accuracy: 0.273000\n",
      "lr 1.000000e-05 reg 3.000000e+01 train_accuracy: 0.301653 val_accuracy: 0.291000\n",
      "\n",
      "best validation accuracy achieved during cross-validation: 0.409000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters\n",
    "# accuracy over 0.35 on the validation set.\n",
    "\n",
    "from classifiers.linear_classifier import Softmax\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [9e-7, 1e-6, 3e-6, 9e-6, 1e-5]\n",
    "regularization_strengths = [0, 1e1, 2e1, 3e1]\n",
    "\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        model = Softmax()\n",
    "        model.train(X_train, y_train, learning_rate=lr, reg=reg, num_iters=5000,\n",
    "                   batch_size=200, verbose=True)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        train_accuracy = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_accuracy = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        results[(lr, reg)] = (train_accuracy, val_accuracy)\n",
    "        if val_accuracy > best_val:\n",
    "            best_val = val_accuracy\n",
    "            best_softmax = model\n",
    "            \n",
    "        print('lr %e reg %e train_accuracy: %f val_accuracy: %f' % (lr, reg, train_accuracy, val_accuracy))\n",
    "    print\n",
    "    \n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train_accuracy: %f val_accuracy: %f' % (lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print\n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 9.000000e-07 reg 0.000000e+00 train_accuracy: 0.395347 val_accuracy: 0.354000\n",
      "lr 9.000000e-07 reg 1.000000e+01 train_accuracy: 0.394551 val_accuracy: 0.355000\n",
      "lr 9.000000e-07 reg 2.000000e+01 train_accuracy: 0.402776 val_accuracy: 0.390000\n",
      "lr 9.000000e-07 reg 3.000000e+01 train_accuracy: 0.406041 val_accuracy: 0.384000\n",
      "lr 1.000000e-06 reg 0.000000e+00 train_accuracy: 0.398714 val_accuracy: 0.391000\n",
      "lr 1.000000e-06 reg 1.000000e+01 train_accuracy: 0.404694 val_accuracy: 0.376000\n",
      "lr 1.000000e-06 reg 2.000000e+01 train_accuracy: 0.408184 val_accuracy: 0.386000\n",
      "lr 1.000000e-06 reg 3.000000e+01 train_accuracy: 0.410796 val_accuracy: 0.380000\n",
      "lr 3.000000e-06 reg 0.000000e+00 train_accuracy: 0.422204 val_accuracy: 0.367000\n",
      "lr 3.000000e-06 reg 1.000000e+01 train_accuracy: 0.429755 val_accuracy: 0.393000\n",
      "lr 3.000000e-06 reg 2.000000e+01 train_accuracy: 0.436898 val_accuracy: 0.398000\n",
      "lr 3.000000e-06 reg 3.000000e+01 train_accuracy: 0.428061 val_accuracy: 0.409000\n",
      "lr 9.000000e-06 reg 0.000000e+00 train_accuracy: 0.386388 val_accuracy: 0.342000\n",
      "lr 9.000000e-06 reg 1.000000e+01 train_accuracy: 0.321143 val_accuracy: 0.290000\n",
      "lr 9.000000e-06 reg 2.000000e+01 train_accuracy: 0.328653 val_accuracy: 0.322000\n",
      "lr 9.000000e-06 reg 3.000000e+01 train_accuracy: 0.288082 val_accuracy: 0.285000\n",
      "lr 1.000000e-05 reg 0.000000e+00 train_accuracy: 0.366286 val_accuracy: 0.341000\n",
      "lr 1.000000e-05 reg 1.000000e+01 train_accuracy: 0.384082 val_accuracy: 0.332000\n",
      "lr 1.000000e-05 reg 2.000000e+01 train_accuracy: 0.289143 val_accuracy: 0.273000\n",
      "lr 1.000000e-05 reg 3.000000e+01 train_accuracy: 0.301653 val_accuracy: 0.291000\n",
      "\n",
      "best validation accuracy achieved during cross-validation: 0.409000\n"
     ]
    }
   ],
   "source": [
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train_accuracy: %f val_accuracy: %f' % (lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print\n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.338000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7ee6969e75ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize the learned weights for each class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# STRIP OUT THE BIAS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mw_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'plane'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'car'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bird'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'frog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'horse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ship'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'truck'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_svm' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize the learned weights for each class.\n",
    "w = best_softmax.W[:-1, :] # STRIP OUT THE BIAS\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    #Rescale the weights to be between 0 and 255\n",
    "    wing = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wing.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
